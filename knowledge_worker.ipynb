{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7bd56e57-765c-47d1-8130-2e4bdbdbe475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902756d7-42de-4940-a6c9-f02759896101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredWordDocumentLoader,UnstructuredExcelLoader, TextLoader, PyPDFLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba37242-8536-4695-b7f6-d9781030c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b677d-3aa7-4d1f-bc9c-96e0c2314adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb866d9-4650-4082-ae35-83cd4f4f942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"mistralai/mistral-small-3.2-24b-instruct:free\"\n",
    "db_name = \"ken_vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21e64440-5a3b-450c-b07f-67866908c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Markdown and text files...\n",
      "Loading PDF files...\n",
      "Loading Word documents...\n",
      "Loading Excel documents...\n",
      "Processing: Hotel_Inventory_Tracking.xlsx\n",
      "Finished loading. Found a total of 701 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Set the path to the folder containing your files\n",
    "folder = \"my_knowledge_base/\"\n",
    "\n",
    "# --- Install Required Libraries ---\n",
    "# You'll need to install the loaders for each file type.\n",
    "# For this example, you would run:\n",
    "# pip install pypdf unstructured python-docx\n",
    "\n",
    "# --- Create Loaders for Each File Type ---\n",
    "# 1. Loader for Markdown (.md) and Text (.txt) files\n",
    "print(\"Loading Markdown and text files...\")\n",
    "\n",
    "\n",
    "txt_loader = DirectoryLoader(\n",
    "    folder,\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "md_loader = DirectoryLoader(\n",
    "    folder,\n",
    "    glob=\"**/*.md\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "# 2. Loader for PDF files\n",
    "print(\"Loading PDF files...\")\n",
    "pdf_loader = DirectoryLoader(\n",
    "    folder,\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "\n",
    "# 3. Loader for Word documents (.docx)\n",
    "print(\"Loading Word documents...\")\n",
    "docx_loader = DirectoryLoader(\n",
    "    folder,\n",
    "    glob=\"**/*.docx\",\n",
    "    loader_cls=UnstructuredWordDocumentLoader\n",
    ")\n",
    "\n",
    "#4. Loader for Word Excel documents (.xlsx)\n",
    "\n",
    "# print(\"Loading Excel documents...\")\n",
    "# xlsx_loader = DirectoryLoader(\n",
    "#     folder,\n",
    "#     glob=\"**/*.xlsx\",\n",
    "#     loader_cls=UnstructuredExcelLoader\n",
    "# )\n",
    "\n",
    "print(\"Loading Excel documents...\")\n",
    "xlsx_files = glob.glob(os.path.join(folder, \"**/*.xlsx\"), recursive=True)\n",
    "pandas_docs = []\n",
    "\n",
    "for xlsx_file in xlsx_files:\n",
    "    print(f\"Processing: {os.path.basename(xlsx_file)}\")\n",
    "    df = pd.read_excel(xlsx_file)\n",
    "    \n",
    "    # Method 1: Create structured text for each row\n",
    "    for index, row in df.iterrows():\n",
    "        # Create a nicely formatted text representation of each row\n",
    "        content = \"\"\n",
    "        for col in df.columns:\n",
    "            if pd.notna(row[col]):  # Only include non-null values\n",
    "                content += f\"{col}: {row[col]}\\n\"\n",
    "        \n",
    "        # Create a document for each row\n",
    "        doc = Document(\n",
    "            page_content=content.strip(),\n",
    "            metadata={\n",
    "                \"source\": xlsx_file,\n",
    "                \"row_index\": index\n",
    "            }\n",
    "        )\n",
    "        pandas_docs.append(doc)\n",
    "\n",
    "# --- Load Documents from All Sources ---\n",
    "# Initialize a list to hold all documents\n",
    "all_documents = []\n",
    "\n",
    "# Load documents from each loader and add them to the list\n",
    "all_documents.extend(txt_loader.load())\n",
    "all_documents.extend(md_loader.load())\n",
    "all_documents.extend(pdf_loader.load())\n",
    "all_documents.extend(docx_loader.load())\n",
    "#all_documents.extend(xlsx_loader.load())\n",
    "all_documents.extend(pandas_docs) \n",
    "\n",
    "print(f\"Finished loading. Found a total of {len(all_documents)} documents.\")\n",
    "\n",
    "# Now, 'all_documents' contains all the content from your files,\n",
    "# ready to be processed for your knowledge worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9861b4-778b-41bb-b672-1ff32a25effc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0a1f349-e924-4470-8784-33fc91c66fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2924, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330823bf-25fb-4bf1-a9c6-705a93920f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93361efc-2327-4844-95a7-e04d418bc3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KENNETH MARK\\AppData\\Local\\Temp\\ipykernel_4204\\1472742473.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 747 documents\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798a442-a5ad-458b-a431-03533c494f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdc5a957-0cfa-42d3-a3a0-a1fa46235a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad4438-41b2-46da-90c8-826583e5302e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65c7ab22-61aa-40f8-b384-0f6d03085947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler # For debugging purposes\n",
    "#For online inference\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,    # or any model OpenRouter supports\n",
    "    api_key=api_key,        # note: it's 'api_key' not 'openai_api_key'\n",
    "    base_url=\"https://openrouter.ai/api/v1\",   # note: it's 'base_url' not 'openai_api_base'\n",
    "    max_tokens=1000   # instead of default 100000\n",
    ")\n",
    "\n",
    "# # For local inference (No need to spend money)\n",
    "# llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "\n",
    "# set up the conversation memory for the chat\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()#search_kwargs={\"k\": 25}\n",
    "\n",
    "# putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)#,\n",
    "                                                          #callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "query = \"Tell me about yam portioning\"\n",
    "result = conversation_chain.invoke({\"question\": query})\n",
    "answer = result[\"answer\"]\n",
    "#print(\"\\nAnswer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ce372-bf55-440c-a0c7-4c13f9b73523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa44d624-aeb9-4924-b24a-ab9764b88aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping in a function - note that history isn't used, as the memory is in the conversation_chain\n",
    "\n",
    "def chat(message, history):\n",
    "    result = conversation_chain.invoke({\"question\": message})\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8799cf-d5fd-4233-b00f-acd04f7db048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bea5c6d-864e-4fbb-9d94-4a3e8797a9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "view = gr.ChatInterface(chat, type=\"messages\")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd5f18ea-0760-42d0-ba46-e501afd97011",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d1c3ce0-090c-4ecb-a808-3937f34ff974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc615d-fce6-4dbd-a420-d2c06a04e438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae633eb5-f1ae-415b-a277-766cee7dd0b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c511499-9996-4a6a-9e54-2a00198c0a37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c819179-03f7-47ee-81bd-d20a3bc6f313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
