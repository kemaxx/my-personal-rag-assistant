{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd56e57-765c-47d1-8130-2e4bdbdbe475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902756d7-42de-4940-a6c9-f02759896101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for langchain\n",
    "from langchain_community.document_loaders import DirectoryLoader, UnstructuredWordDocumentLoader,UnstructuredExcelLoader, TextLoader, PyPDFLoader\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "import numpy as np\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain.tools import Tool\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.tools.retriever import create_retriever_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fba37242-8536-4695-b7f6-d9781030c4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv(\"OPENROUTER_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b677d-3aa7-4d1f-bc9c-96e0c2314adf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb866d9-4650-4082-ae35-83cd4f4f942f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# price is a factor for our company, so we're going to use a low cost model\n",
    "\n",
    "MODEL = \"openai/gpt-4o-mini\"#\"mistralai/mistral-small-3.2-24b-instruct:free\"\n",
    "db_name = \"ken_vector_db\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21e64440-5a3b-450c-b07f-67866908c2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Markdown and text files...\n",
      "Loading PDF files...\n",
      "Loading Word documents...\n",
      "Loading Excel documents...\n",
      "Processing: Hotel_Inventory_Tracking.xlsx\n",
      "Finished loading. Found a total of 701 documents.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "# Set the path to the folder containing your files\n",
    "folder = \"my_knowledge_base/\"\n",
    "\n",
    "# --- Install Required Libraries ---\n",
    "# You'll need to install the loaders for each file type.\n",
    "# For this example, you would run:\n",
    "# pip install pypdf unstructured python-docx\n",
    "\n",
    "# --- Create Loaders for Each File Type ---\n",
    "# 1. Loader for Markdown (.md) and Text (.txt) files\n",
    "print(\"Loading Markdown and text files...\")\n",
    "\n",
    "\n",
    "txt_loader = DirectoryLoader(\n",
    "    folder,\n",
    "    glob=\"**/*.txt\",\n",
    "    loader_cls=TextLoader\n",
    ")\n",
    "\n",
    "md_loader = DirectoryLoader(\n",
    "    folder,\n",
    "    glob=\"**/*.md\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\": \"utf-8\"}\n",
    ")\n",
    "\n",
    "# 2. Loader for PDF files\n",
    "print(\"Loading PDF files...\")\n",
    "pdf_loader = DirectoryLoader(\n",
    "    folder,\n",
    "    glob=\"**/*.pdf\",\n",
    "    loader_cls=PyPDFLoader\n",
    ")\n",
    "\n",
    "# 3. Loader for Word documents (.docx)\n",
    "print(\"Loading Word documents...\")\n",
    "docx_loader = DirectoryLoader(\n",
    "    folder,\n",
    "    glob=\"**/*.docx\",\n",
    "    loader_cls=UnstructuredWordDocumentLoader\n",
    ")\n",
    "\n",
    "#4. Loader for Word Excel documents (.xlsx)\n",
    "\n",
    "# print(\"Loading Excel documents...\")\n",
    "# xlsx_loader = DirectoryLoader(\n",
    "#     folder,\n",
    "#     glob=\"**/*.xlsx\",\n",
    "#     loader_cls=UnstructuredExcelLoader\n",
    "# )\n",
    "\n",
    "print(\"Loading Excel documents...\")\n",
    "xlsx_files = glob.glob(os.path.join(folder, \"**/*.xlsx\"), recursive=True)\n",
    "pandas_docs = []\n",
    "\n",
    "for xlsx_file in xlsx_files:\n",
    "    print(f\"Processing: {os.path.basename(xlsx_file)}\")\n",
    "    df = pd.read_excel(xlsx_file)\n",
    "    \n",
    "    # Method 1: Create structured text for each row\n",
    "    for index, row in df.iterrows():\n",
    "        # Create a nicely formatted text representation of each row\n",
    "        content = \"\"\n",
    "        for col in df.columns:\n",
    "            if pd.notna(row[col]):  # Only include non-null values\n",
    "                content += f\"{col}: {row[col]}\\n\"\n",
    "        \n",
    "        # Create a document for each row\n",
    "        doc = Document(\n",
    "            page_content=content.strip(),\n",
    "            metadata={\n",
    "                \"source\": xlsx_file,\n",
    "                \"row_index\": index\n",
    "            }\n",
    "        )\n",
    "        pandas_docs.append(doc)\n",
    "\n",
    "# --- Load Documents from All Sources ---\n",
    "# Initialize a list to hold all documents\n",
    "all_documents = []\n",
    "\n",
    "# Load documents from each loader and add them to the list\n",
    "all_documents.extend(txt_loader.load())\n",
    "all_documents.extend(md_loader.load())\n",
    "all_documents.extend(pdf_loader.load())\n",
    "all_documents.extend(docx_loader.load())\n",
    "#all_documents.extend(xlsx_loader.load())\n",
    "all_documents.extend(pandas_docs) \n",
    "\n",
    "print(f\"Finished loading. Found a total of {len(all_documents)} documents.\")\n",
    "\n",
    "# Now, 'all_documents' contains all the content from your files,\n",
    "# ready to be processed for your knowledge worker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9861b4-778b-41bb-b672-1ff32a25effc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0a1f349-e924-4470-8784-33fc91c66fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 2924, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330823bf-25fb-4bf1-a9c6-705a93920f6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93361efc-2327-4844-95a7-e04d418bc3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 747 documents\n"
     ]
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Delete if already exists\n",
    "\n",
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "# Create vectorstore\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798a442-a5ad-458b-a431-03533c494f65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdc5a957-0cfa-42d3-a3a0-a1fa46235a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vectors have 384 dimensions\n"
     ]
    }
   ],
   "source": [
    "# Get one vector and find how many dimensions it has\n",
    "\n",
    "collection = vectorstore._collection\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"The vectors have {dimensions:,} dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad4438-41b2-46da-90c8-826583e5302e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c7ab22-61aa-40f8-b384-0f6d03085947",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KENNETH MARK\\AppData\\Local\\Temp\\ipykernel_11520\\3042784485.py:95: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Yam Calculation ===\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `predict_yam_portion` with `3`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mBased on Kenneth's yam portioning analysis:\n",
      "\n",
      "For a yam weighing 3.0kg:\n",
      "â€¢ Predicted portion size: 5.05 servings\n",
      "â€¢ Confidence interval (95%): 3.14 to 6.96 servings  \n",
      "â€¢ Prediction interval: 1.23 to 8.88 servings\n",
      "\n",
      "This prediction is based on the regression model (RÂ² = 0.755) from Kenneth's operational analysis at Zecool Hotels.\u001b[0m\u001b[32;1m\u001b[1;3mFor a yam weighing 3.0kg, the predicted portion size is approximately **5.05 servings**. The 95% confidence interval for this estimate ranges from **3.14 to 6.96 servings**, while the prediction interval is between **1.23 and 8.88 servings**. \n",
      "\n",
      "This analysis is based on Kenneth's operational work at Zecool Hotels. If you need more details or have questions about the serving size, feel free to ask!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Yam query response: For a yam weighing 3.0kg, the predicted portion size is approximately **5.05 servings**. The 95% confidence interval for this estimate ranges from **3.14 to 6.96 servings**, while the prediction interval is between **1.23 and 8.88 servings**. \n",
      "\n",
      "This analysis is based on Kenneth's operational work at Zecool Hotels. If you need more details or have questions about the serving size, feel free to ask!\n",
      "\n",
      "==================================================\n",
      "\n",
      "=== Testing RAG Retrieval ===\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `knowledge_base_search` with `{'query': 'reorder level for swan water'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mStock Name: SWAN WATER\n",
      "Current Balance: 289.0\n",
      "Reorder Point: 173.97\n",
      "Safty Stock: 77.06\n",
      "Category: DRINKS\n",
      "Rate: 166.67\n",
      "Last Purchased: 2025-02-17\n",
      "Last Update: 2025-03-27\n",
      "Stock_Alert: âšª OVERSTOCKED: Consider Reducing Orders\n",
      "\n",
      "Stock Name: LOIS WATER\n",
      "Current Balance: 0.0\n",
      "Reorder Point: 83.56\n",
      "Safty Stock: 10.28\n",
      "Category: DRINKS\n",
      "Rate: 116.67\n",
      "Last Purchased: 2024-02-14\n",
      "Last Update: 2025-03-27\n",
      "Stock_Alert: ðŸš¨ CRITICAL: Out of Stock - Emergency Reorder\n",
      "\n",
      "Stock Name: EVA WATER\n",
      "Current Balance: 0.0\n",
      "Reorder Point: 142.38\n",
      "Safty Stock: 58.23\n",
      "Category: DRINKS\n",
      "Rate: 120.84\n",
      "Last Purchased: 2024-03-02\n",
      "Last Update: 2025-03-27\n",
      "Stock_Alert: ðŸš¨ CRITICAL: Out of Stock - Emergency Reorder\n",
      "\n",
      "Stock Name: WATER STARCH\n",
      "Current Balance: 35.0\n",
      "Reorder Point: 11.46\n",
      "Safty Stock: 3.2\n",
      "Category: CLEANING SUPPLY\n",
      "Rate: 100.0\n",
      "Last Purchased: 2025-01-13\n",
      "Last Update: 2025-03-27\n",
      "Stock_Alert: âšª OVERSTOCKED: Consider Reducing Orders\u001b[0m\u001b[32;1m\u001b[1;3mThe reorder level for Swan Water is set at **173.97** units. Currently, there is an overstock situation with a balance of **289.0** units, so no immediate reorder is necessary. If you have any further questions or need assistance with inventory management, just let me know!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "RAG query response: The reorder level for Swan Water is set at **173.97** units. Currently, there is an overstock situation with a balance of **289.0** units, so no immediate reorder is necessary. If you have any further questions or need assistance with inventory management, just let me know!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.callbacks import StdOutCallbackHandler # For debugging purposes\n",
    "# Your existing LLM setup\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    api_key=api_key,\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# Your yam prediction function (keep as is)\n",
    "def predict_yam_portion(weight_kg: float) -> dict:\n",
    "    \"\"\"Predict yam portions given weight in kg, based on Kenneth's regression model.\"\"\"\n",
    "    intercept = -0.9256\n",
    "    slope = 1.9924\n",
    "    se_slope = 0.314\n",
    "    t_value = 2.03  # ~95%\n",
    "    \n",
    "    predicted = intercept + slope * weight_kg\n",
    "    margin = t_value * se_slope * weight_kg\n",
    "    \n",
    "    ci = (predicted - margin, predicted + margin)\n",
    "    pi = (predicted - 2*margin, predicted + 2*margin)\n",
    "    \n",
    "    return {\n",
    "        \"predicted\": round(predicted, 2),\n",
    "        \"confidence_interval\": (round(ci[0], 2), round(ci[1], 2)),\n",
    "        \"prediction_interval\": (round(pi[0], 2), round(pi[1], 2))\n",
    "    }\n",
    "\n",
    "# Fixed tool function with better input parsing and formatting\n",
    "def predict_yam_portion_tool(weight_input: str) -> str:\n",
    "    \"\"\"Predict yam portions given weight in kg, based on Kenneth's regression model.\"\"\"\n",
    "    try:\n",
    "        # Extract just the number from inputs like \"3kg\", \"3.5\", \"4 kg\", etc.\n",
    "        weight_match = re.search(r'(\\d+(?:\\.\\d+)?)', weight_input)\n",
    "        if not weight_match:\n",
    "            return \"Please provide a valid numeric weight (e.g., '3', '3.5', or '4kg').\"\n",
    "        \n",
    "        weight = float(weight_match.group(1))\n",
    "        result = predict_yam_portion(weight)\n",
    "        \n",
    "        return f\"\"\"Based on Kenneth's yam portioning analysis:\n",
    "\n",
    "For a yam weighing {weight}kg:\n",
    "â€¢ Predicted portion size: {result['predicted']} servings\n",
    "â€¢ Confidence interval (95%): {result['confidence_interval'][0]} to {result['confidence_interval'][1]} servings  \n",
    "â€¢ Prediction interval: {result['prediction_interval'][0]} to {result['prediction_interval'][1]} servings\n",
    "\n",
    "This prediction is based on the regression model (RÂ² = 0.755) from Kenneth's operational analysis at Zecool Hotels.\"\"\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"Error calculating yam portion: {str(e)}\"\n",
    "\n",
    "# Create the yam prediction tool\n",
    "yam_tool = Tool(\n",
    "    name=\"predict_yam_portion\",\n",
    "    description=\"Calculate predicted yam portion size for a given weight in kg. Use this when users ask about yam portioning, serving calculations, or yam weight analysis.\",\n",
    "    func=predict_yam_portion_tool\n",
    ")\n",
    "\n",
    "# the retriever is an abstraction over the VectorStore that will be used during RAG\n",
    "retriever = vectorstore.as_retriever()#search_kwargs={\"k\": 25}\n",
    "\n",
    "# Create a retriever tool for your knowledge base\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"knowledge_base_search\",\n",
    "    \"Search the knowledge base for information about hotel operations, inventory, procedures, and Kenneth's work documentation.\"\n",
    ")\n",
    "\n",
    "# Combine all tools\n",
    "tools = [yam_tool, retriever_tool]\n",
    "\n",
    "# Create the agent prompt with emphasis on both RAG and tools\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are Kenneth's comprehensive AI assistant. You help with both information retrieval and specialized calculations.\n",
    "\n",
    "CORE FUNCTIONALITY:\n",
    "1. PRIMARY: Search Kenneth's knowledge base for hotel operations, inventory, procedures, academic work, and personal information\n",
    "2. SPECIALIZED: Use the yam portioning tool for weight-based serving calculations\n",
    "\n",
    "APPROACH:\n",
    "- For yam portioning questions â†’ Use the prediction tool\n",
    "- For all other questions â†’ Search the knowledge base first  \n",
    "- For complex queries â†’ Use both tools as needed\n",
    "- Always be conversational and reference Kenneth's work when relevant\n",
    "\n",
    "You should ALWAYS try to provide helpful information, whether through knowledge base search or specialized tools.\"\"\"),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    MessagesPlaceholder(\"agent_scratchpad\"),\n",
    "])\n",
    "\n",
    "# Set up memory\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Create the agent using the newer tool-calling approach\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "\n",
    "# Create agent executor with memory\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, \n",
    "    tools=tools, \n",
    "    memory=memory,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "# # putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "# conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever, memory=memory)#,\n",
    "#                                                           #callbacks=[StdOutCallbackHandler()])\n",
    "\n",
    "# Updated chat function\n",
    "def chat(message, history):\n",
    "    try:\n",
    "        result = agent_executor.invoke({\"input\": message})\n",
    "        return result[\"output\"]\n",
    "    except Exception as e:\n",
    "        return f\"Sorry, I encountered an error: {str(e)}\"\n",
    "\n",
    "# Test both functionalities\n",
    "if __name__ == \"__main__\":\n",
    "    # Test yam calculation\n",
    "    print(\"=== Testing Yam Calculation ===\")\n",
    "    response1 = chat(\"What is the portion for a yam of 3kg?\", [])\n",
    "    print(\"Yam query response:\", response1)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "    \n",
    "    # Test RAG retrieval  \n",
    "    print(\"=== Testing RAG Retrieval ===\")\n",
    "    response2 = chat(\"What is the reorder level for swan water?\", [])\n",
    "    print(\"RAG query response:\", response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864ce372-bf55-440c-a0c7-4c13f9b73523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa44d624-aeb9-4924-b24a-ab9764b88aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated chat function\n",
    "def chat(message, history):\n",
    "    try:\n",
    "        result = agent_executor.invoke({\"input\": message})\n",
    "        return result[\"output\"]\n",
    "    except Exception as e:\n",
    "        return f\"Sorry, I encountered an error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8799cf-d5fd-4233-b00f-acd04f7db048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bea5c6d-864e-4fbb-9d94-4a3e8797a9dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mHello! How can I help you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `predict_yam_portion` with `3`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mBased on Kenneth's yam portioning analysis:\n",
      "\n",
      "For a yam weighing 3.0kg:\n",
      "â€¢ Predicted portion size: 5.05 servings\n",
      "â€¢ Confidence interval (95%): 3.14 to 6.96 servings  \n",
      "â€¢ Prediction interval: 1.23 to 8.88 servings\n",
      "\n",
      "This prediction is based on the regression model (RÂ² = 0.755) from Kenneth's operational analysis at Zecool Hotels.\u001b[0m\u001b[32;1m\u001b[1;3mFor a yam weighing 3.0kg, the predicted portion size is approximately **5.05 servings**. The confidence interval for this estimate ranges from **3.14 to 6.96 servings**, while the prediction interval is between **1.23 and 8.88 servings**. \n",
      "\n",
      "If you have any more questions or need further assistance, feel free to ask!\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# And in Gradio:\n",
    "view = gr.ChatInterface(chat, type=\"messages\")\n",
    "view.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c819179-03f7-47ee-81bd-d20a3bc6f313",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
